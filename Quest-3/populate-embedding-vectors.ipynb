{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Populating Embedding Vectors in MongoDB Atlas\n",
    "\n",
    "In this Python notebook, we'll be using the embedding models we've downloaded to our local device to create embedding attributes for our movies dataset. Once we've done that, we'll be using `pymongo` to add these new embedding attributes to our dataset in MongoDB.\n",
    "\n",
    "The cool thing about what we're doing here is that we're generating all the embeddings locally (i.e. no external API calls needed); so if you have your own custom embedding models, you can make use of that for other projects too! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Settings\n",
    "We start by loading up some environment settings. These should all have been configured in Quest 1, so head to Quest 1 if you're missing out anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATLAS_URI Connection string found: mongodb+srv://stackies:nIR5ez8mrtgoBeaH@cluster0.w34yhfu.mongodb.net/\n"
     ]
    }
   ],
   "source": [
    "# Load settings from .env file\n",
    "import sys\n",
    "from dotenv import find_dotenv, dotenv_values\n",
    "\n",
    "# Change system path to root direcotry\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "config = dotenv_values(find_dotenv())\n",
    "\n",
    "# For debugging purposes\n",
    "# print (config)\n",
    "\n",
    "ATLAS_URI = config.get('ATLAS_URI')\n",
    "\n",
    "if not ATLAS_URI:\n",
    "    raise Exception (\"'ATLAS_URI' is not set.  Please set it above to continue...\")\n",
    "else:\n",
    "    print(\"ATLAS_URI Connection string found:\", ATLAS_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our variables\n",
    "DB_NAME = 'sample_mflix'\n",
    "COLLECTION_NAME = 'embedded_movies'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize Mongo Atlas Client\n",
    "Then, we intialize a connection to Mongo Atlas Client by using our unique ATLAS_URI value. Once we've succesfully connected, we'll be able to interact with our MongoDB database. For example, in the second code cell, we're getting our embedded_movies dataset and printing out its document count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the Mongo Atlas database!\n"
     ]
    }
   ],
   "source": [
    "from AtlasClient import AtlasClient\n",
    "\n",
    "atlas_client = AtlasClient (ATLAS_URI, DB_NAME)\n",
    "print(\"Connected to the Mongo Atlas database!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document count = 3,483 movies\n"
     ]
    }
   ],
   "source": [
    "collection = atlas_client.get_collection(COLLECTION_NAME)\n",
    "document_count = collection.count_documents({})\n",
    "\n",
    "print (f\"Document count = {document_count:,} movies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate Embeddings\n",
    "Now for the fun part - we're going to generate all embeddings locally on our computer, using open source models. No API calls or API KEYS needed! ðŸ˜„\n",
    "\n",
    "As mentioned, we'll be using the following models:\n",
    "\n",
    "| model name                              | overall score | model params | model size | embedding length | url                                                            |\n",
    "|-----------------------------------------|---------------|--------------|------------|------------------|----------------------------------------------------------------|\n",
    "| BAAI/bge-small-en-v1.5                  | 62.x          | 33.5 M       | 133 MB     | 384              | https://huggingface.co/BAAI/bge-small-en-v1.5                  |\n",
    "| sentence-transformers/all-mpnet-base-v2 | 57.8          |              | 438 MB     | 768              | https://huggingface.co/sentence-transformers/all-mpnet-base-v2 |\n",
    "| sentence-transformers/all-MiniLM-L6-v2  | 56.x          |              | 91 MB      | 384              | https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# LlamaIndex will download embeddings models as needed\n",
    "# Set llamaindex cache dir to ../cache dir here (Default is system tmp)\n",
    "# This way, we can easily see downloaded artifacts\n",
    "os.environ['LLAMA_INDEX_CACHE_DIR'] = os.path.join(os.path.abspath('../'), 'cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "#from llama_index.embeddings import HuggingFaceEmbedding\n",
    "import time\n",
    "\n",
    "# Helper function to calculate embeddings, given a model\n",
    "def create_embeddings (movies, embedding_model, embedding_attr):\n",
    "    embed_model = HuggingFaceEmbedding(model_name=embedding_model)\n",
    "\n",
    "    t2a = time.perf_counter()\n",
    "    for movie in movies:\n",
    "        movie[embedding_attr] = embed_model.get_text_embedding(movie['plot'])\n",
    "\n",
    "    t2b = time.perf_counter()\n",
    "    # print (f'Embeddings generated for {len(movies):,} movies  in {(t2b-t2a)*1000:,.0f} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 3,403 from Atlas in 24,365 ms\n"
     ]
    }
   ],
   "source": [
    "# Fetch all movies\n",
    "t1a = time.perf_counter()\n",
    "movies = [m for m in atlas_client.find (collection_name=COLLECTION_NAME, filter={'plot':{\"$exists\": True}}, limit=0)]\n",
    "t1b = time.perf_counter()\n",
    "\n",
    "print (f'Fetched {len(movies):,} from Atlas in {(t1b-t1a)*1000:,.0f} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding models we want to use\n",
    "model_mappings = {\n",
    "    'BAAI/bge-small-en-v1.5' : {'embedding_attr' : 'plot_embedding_bge_small', 'index_name' : 'idx_plot_embedding_bge_small'},\n",
    "    'sentence-transformers/all-mpnet-base-v2' : {'embedding_attr' : 'plot_embedding_mpnet_base_v2', 'index_name' : 'idx_plot_embedding_mpnet_base_v2'},\n",
    "    'sentence-transformers/all-MiniLM-L6-v2' : {'embedding_attr' : 'plot_embedding_minilm_l6_v2', 'index_name' : 'idx_plot_embedding_minilm_l6_v2'},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------- Embedding Model = BAAI/bge-small-en-v1.5 ---------\n",
      "model=BAAI/bge-small-en-v1.5, created embeddings for 3,403 movies in 1,219,040 ms, avg_time_per_movie=358 ms\n",
      "\n",
      "------- Embedding Model = sentence-transformers/all-mpnet-base-v2 ---------\n",
      "model=sentence-transformers/all-mpnet-base-v2, created embeddings for 3,403 movies in 262,977 ms, avg_time_per_movie=77 ms\n",
      "\n",
      "------- Embedding Model = sentence-transformers/all-MiniLM-L6-v2 ---------\n",
      "model=sentence-transformers/all-MiniLM-L6-v2, created embeddings for 3,403 movies in 689,534 ms, avg_time_per_movie=203 ms\n"
     ]
    }
   ],
   "source": [
    "# For selected embedding models above, we are going to create vectors for the plot field\n",
    "# Each embedding model will have its own 'plot_embedding' attribute (i.e. we don't want to mix them up)\n",
    "# It might take up to a few minutes to generate the embeddings\n",
    "\n",
    "for key in model_mappings.keys():\n",
    "    embedding_model = key\n",
    "    embedding_attr = model_mappings[key]['embedding_attr']\n",
    "\n",
    "    print (f'\\n------- Embedding Model = {embedding_model} ---------')\n",
    "    t1a = time.perf_counter()\n",
    "    create_embeddings(movies=movies, embedding_model=embedding_model, embedding_attr=embedding_attr)\n",
    "    t1b = time.perf_counter()\n",
    "    avg_time_per_movie = (t1b-t1a)*1000 / len(movies)\n",
    "    print (f'model={embedding_model}, created embeddings for {len(movies):,} movies in {(t1b-t1a)*1000:,.0f} ms, avg_time_per_movie={avg_time_per_movie:,.0f} ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Inspect Generated Embeddings\n",
    "Great job! At this point, we should have succesfully generated 3 sets of embeddings, 1 set for each embedding model we used. Let's try running the code cell below a few times to see the embeddings generated by the different embedding models for a given random movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selected movie:  Mighty Morphin Power Rangers: The Movie\n",
      "Movie plot:  A giant egg is unearthed at a construction site and soon opened, releasing the terrible Ivan Ooze, who wreaks vengeance on Zordon for imprisoning him millennia ago. With Zordon dying and their powers lost, the Rangers head to a distant planet to find the mystic warrior Dulcea. \n",
      "\n",
      "plot_embeddings (existing openAI generated), len=1536 , [0.008537359, -0.0419179, -0.016995177, -0.0045039873, -0.030145891]...\n",
      "\n",
      "plot_embedding_bge_small , len=384 , [-0.04320862516760826, -0.014960034750401974, 0.005376668181270361, 0.04989439994096756, 0.0363854356110096]...\n",
      "\n",
      "plot_embedding_mpnet_base_v2 , len=768 , [0.07135888189077377, 0.0057554468512535095, 0.022949937731027603, 0.01702651008963585, 0.02376585453748703]...\n",
      "\n",
      "plot_embedding_minilm_l6_v2 , len=384 , [-0.08905382454395294, 0.0890040472149849, 0.0572551004588604, 0.009834816679358482, 0.04117322340607643]...\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "movie = random.choice(movies)\n",
    "print ('Randomly selected movie: ', movie['title'])\n",
    "print ('Movie plot: ', movie['plot'], '\\n')\n",
    "print (f'plot_embeddings (existing openAI generated), len={len(movie[\"plot_embedding\"])} , {movie[\"plot_embedding\"][:5]}...\\n')\n",
    "print (f'plot_embedding_bge_small , len={len(movie[\"plot_embedding_bge_small\"])} , {movie[\"plot_embedding_bge_small\"][:5]}...\\n')\n",
    "print (f'plot_embedding_mpnet_base_v2 , len={len(movie[\"plot_embedding_mpnet_base_v2\"])} , {movie[\"plot_embedding_mpnet_base_v2\"][:5]}...\\n')\n",
    "print (f'plot_embedding_minilm_l6_v2 , len={len(movie[\"plot_embedding_minilm_l6_v2\"])} , {movie[\"plot_embedding_minilm_l6_v2\"][:5]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Add Embeddings to MongoDB Atlas\n",
    "Now that we've generated all our embeddings, let's go ahead and add them into our dataset in MongoDB Atlas! We'll be using a bulk update approach to save some time in uploading our embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to update 3403 movies in Atlas...\n",
      "Update matched count: 3403\n",
      "Update modified count: 3403\n",
      "Updated 3,403 in Atlas in 31,703 ms\n"
     ]
    }
   ],
   "source": [
    "from pymongo import ReplaceOne\n",
    "\n",
    "collection = atlas_client.get_collection(COLLECTION_NAME)\n",
    "replacements = [ReplaceOne ({\"_id\" : movie[\"_id\"]}, movie) for movie in movies]\n",
    "\n",
    "# Perform bulk replacement\n",
    "print (f'About to update {len(replacements)} movies in Atlas...')\n",
    "t1a = time.perf_counter()\n",
    "result = collection.bulk_write(replacements)\n",
    "t1b = time.perf_counter()\n",
    "\n",
    "# Print result\n",
    "print(f\"Update matched count: {result.matched_count}\")\n",
    "print(f\"Update modified count: {result.modified_count}\")\n",
    "print (f'Updated {len(movies):,} in Atlas in {(t1b-t1a)*1000:,.0f} ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're done with this notebook! Please **head back to the Quest page on StackUp now** where we'll be proceeding with the next step - creating a search index for each of our newly created custom embeddings. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas-1",
   "language": "python",
   "name": "atlas-1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
